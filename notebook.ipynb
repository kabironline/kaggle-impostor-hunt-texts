{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9a45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7b3bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utils.vector_db import VectorDB\n",
    "from chromadb import EmbeddingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7964564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 3070\n",
      "CUDA Version: 12.6\n",
      "Available GPU memory: 8.0 GB\n",
      "Model loaded with FP16 precision for faster GPU processing\n",
      "Model loaded on: cuda:0\n",
      "Model loaded with FP16 precision for faster GPU processing\n",
      "Model loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer with GPU support\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Enable mixed precision for faster training (if GPU supports it)\n",
    "if torch.cuda.is_available():\n",
    "    model.half()  # Use FP16 for faster inference\n",
    "    print(\"Model loaded with FP16 precision for faster GPU processing\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")\n",
    "\n",
    "print(f\"Model loaded on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40d065a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40be97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_or_create_paired_df(data_dir, csv_path, has_real=True):\n",
    "    \"\"\"\n",
    "    If csv_path exists -> load it.\n",
    "    Else -> loop through article_* folders in data_dir and build a dataframe with:\n",
    "    - text_1, text_2\n",
    "    - real (only if has_real=True), looked up from <parent_of_data_dir>/train.csv\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        return pd.read_csv(csv_path)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    if has_real:\n",
    "        # load the csv at \"data/train.csv\"\n",
    "        real_df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "    for article_dir in sorted(d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))):\n",
    "        article_path = os.path.join(data_dir, article_dir)\n",
    "        f1 = os.path.join(article_path, \"file_1.txt\")\n",
    "        f2 = os.path.join(article_path, \"file_2.txt\")\n",
    "\n",
    "        text_1 = read_text(f1)\n",
    "        text_2 = read_text(f2)\n",
    "\n",
    "        row = {\"text_1\": text_1, \"text_2\": text_2}\n",
    "\n",
    "        if has_real:\n",
    "            # lookup the \"real\" value from the real_df\n",
    "            real_row = real_df[real_df[\"id\"] == int(article_dir.split(\"_\")[1])]\n",
    "            real_value = real_row[\"real\"].values[0] if not real_row.empty else np.nan\n",
    "            row[\"real\"] = real_value\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Usage\n",
    "train_data_dir = \"data/train\"\n",
    "test_data_dir  = \"data/test\"\n",
    "train_csv = \"data/stored_train_data.csv\"\n",
    "test_csv  = \"data/stored_test_data.csv\"\n",
    "\n",
    "paired_df = load_or_create_paired_df(train_data_dir, train_csv, has_real=True)\n",
    "test_df   = load_or_create_paired_df(test_data_dir,  test_csv,  has_real=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d49d9115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>real</th>\n",
       "      <th>cleaned_text_1</th>\n",
       "      <th>cleaned_text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n",
       "      <td>The China relay network has released a signifi...</td>\n",
       "      <td>1</td>\n",
       "      <td>virsa visible infrared survey telescope array ...</td>\n",
       "      <td>china relay network released significant amoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China\\nThe goal of this project involves achie...</td>\n",
       "      <td>The project aims to achieve an accuracy level ...</td>\n",
       "      <td>2</td>\n",
       "      <td>china goal project involves achieving accuracy...</td>\n",
       "      <td>project aim achieve accuracy level dex analyzi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scientists can learn about how galaxies form a...</td>\n",
       "      <td>Dinosaur eggshells offer clues about what dino...</td>\n",
       "      <td>1</td>\n",
       "      <td>scientist learn galaxy form evolve two method ...</td>\n",
       "      <td>dinosaur eggshell offer clue dinosaur ate long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>China\\nThe study suggests that multiple star s...</td>\n",
       "      <td>The importance for understanding how stars evo...</td>\n",
       "      <td>2</td>\n",
       "      <td>china study suggests multiple star system play...</td>\n",
       "      <td>importance understanding star evolve led resea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dinosaur Rex was excited about his new toy set...</td>\n",
       "      <td>Analyzing how fast stars rotate within a galax...</td>\n",
       "      <td>2</td>\n",
       "      <td>dinosaur rex excited new toy set many dinosaur...</td>\n",
       "      <td>analyzing fast star rotate within galaxy compa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  The VIRSA (Visible Infrared Survey Telescope A...   \n",
       "1  China\\nThe goal of this project involves achie...   \n",
       "2  Scientists can learn about how galaxies form a...   \n",
       "3  China\\nThe study suggests that multiple star s...   \n",
       "4  Dinosaur Rex was excited about his new toy set...   \n",
       "\n",
       "                                              text_2  real  \\\n",
       "0  The China relay network has released a signifi...     1   \n",
       "1  The project aims to achieve an accuracy level ...     2   \n",
       "2  Dinosaur eggshells offer clues about what dino...     1   \n",
       "3  The importance for understanding how stars evo...     2   \n",
       "4  Analyzing how fast stars rotate within a galax...     2   \n",
       "\n",
       "                                      cleaned_text_1  \\\n",
       "0  virsa visible infrared survey telescope array ...   \n",
       "1  china goal project involves achieving accuracy...   \n",
       "2  scientist learn galaxy form evolve two method ...   \n",
       "3  china study suggests multiple star system play...   \n",
       "4  dinosaur rex excited new toy set many dinosaur...   \n",
       "\n",
       "                                      cleaned_text_2  \n",
       "0  china relay network released significant amoun...  \n",
       "1  project aim achieve accuracy level dex analyzi...  \n",
       "2  dinosaur eggshell offer clue dinosaur ate long...  \n",
       "3  importance understanding star evolve led resea...  \n",
       "4  analyzing fast star rotate within galaxy compa...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Join the tokens back into a cleaned string\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def clean_df(df):\n",
    "    df['cleaned_text_1'] = df['text_1'].apply(clean_text)\n",
    "    df['cleaned_text_2'] = df['text_2'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "paired_df = clean_df(paired_df)\n",
    "paired_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7904e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = clean_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abcc0197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kabir\\AppData\\Local\\Temp\\ipykernel_106328\\3752022874.py:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():  # Use automatic mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding shape: torch.Size([1, 9, 768])\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def extract_bert_embeddings(text, device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.amp.autocast():  # Use automatic mixed precision\n",
    "                outputs = model(**inputs)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "        # The last hidden state contains the embeddings\n",
    "        embeddings = outputs.last_hidden_state.cpu()  # Move back to CPU for return\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self, model, tokenizer, device=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device if device is not None else next(model.parameters()).device\n",
    "\n",
    "    def __call__(self, input: list) -> list:\n",
    "        # input: list of strings\n",
    "        embeddings = []\n",
    "        \n",
    "        # Process in batches for better GPU utilization\n",
    "        batch_size = 16 if self.device.type == 'cuda' else 4\n",
    "        \n",
    "        for i in range(0, len(input), batch_size):\n",
    "            batch_texts = input[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if self.device.type == 'cuda':\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(**inputs)\n",
    "                else:\n",
    "                    outputs = self.model(**inputs)\n",
    "                \n",
    "                # Use the [CLS] token embedding as sentence embedding\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                \n",
    "                for emb in cls_embeddings:\n",
    "                    embeddings.append(emb.tolist())\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Test the GPU-accelerated embedding function\n",
    "sample_embedding = extract_bert_embeddings(\"Sample text for embedding.\")\n",
    "print(f\"Sample embedding shape: {sample_embedding.shape}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Clear GPU cache if using CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba716b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = []\n",
    "# for idx, row in paired_df.iterrows():\n",
    "#     if str(row['cleaned_text_1']).strip():\n",
    "#         documents.append({\n",
    "#             \"id\": f\"{idx}_1\",\n",
    "#             \"content\": row['cleaned_text_1'],\n",
    "#             \"metadata\": {\"real\": row[\"real\"] == 1}\n",
    "#         })\n",
    "#     if str(row['cleaned_text_2']).strip():\n",
    "#         documents.append({\n",
    "#             \"id\": f\"{idx}_2\",\n",
    "#             \"content\": row['cleaned_text_2'],\n",
    "#             \"metadata\": {\"real\": row[\"real\"] == 2}\n",
    "#         })\n",
    "\n",
    "# # Delete the existing collection if it exists (to fix dimension mismatch)\n",
    "# rebuild_collection = False\n",
    "# if rebuild_collection:\n",
    "#     vector_db_tmp = VectorDB(\n",
    "#         collection_name=\"impostor_hunt_texts\",\n",
    "#         embedding_length=384,\n",
    "#         working_dir=os.getcwd()\n",
    "#     )\n",
    "#     vector_db_tmp.delete_collection()\n",
    "\n",
    "# embedding_function = MyEmbeddingFunction(model, tokenizer)\n",
    "\n",
    "\n",
    "# # Initialize VectorDB (embedding_function can be left as None to use default)\n",
    "# vector_db = VectorDB(\n",
    "#     collection_name=\"impostor_hunt_texts\",\n",
    "#     embedding_length=768,\n",
    "#     working_dir=os.getcwd(),\n",
    "#     documents=documents,\n",
    "#     dont_add_if_collection_exist=not rebuild_collection\n",
    "# )\n",
    "\n",
    "# vector_db.search(\"\"\"ChromeDriver music player\n",
    "# This study focused on identifying any non-spherical shapes within specific types of celestial bodies (music music) using various techniques like comparing how they look from different directions and analyzing their changes in sound pressure vs time .\n",
    "# The extent to which these artists' images show evidence for an overall shape rather than individual tracks was found across multiple tracks:\n",
    "# Two specific songs had clearly visible distortions due to their complex structure compared to others playing just simple beats\n",
    "# This research found that while most recordings showed a relatively simple structure (like when you only see one instrument rather than an entire grand orchestra), some featured noticeable deviations from those expectations (like if there were multiple instruments playing at once). These results suggest there may be a correlation between how musicians program their compositions and how much curvature they chose for their soundscape — it seems as though tracks with more intricate arrangements tend towards greater complexity!\n",
    "# Please note: This is just an example response based on your input text as I am not able access real world information such as music information or even what \"music music\" means without further context!\n",
    "# Let me know if you want me to try working through some real world examples instead? I can also provide alternative ways I could rephrase your initial statement!\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cca449a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Late Chunking for 'real' and 'not real' groups ---\n",
    "real_docs = []\n",
    "not_real_docs = []\n",
    "for idx, row in paired_df.iterrows():\n",
    "    text_1 = row['cleaned_text_1']\n",
    "    text_2 = row['cleaned_text_2']\n",
    "    # Only process if text_1 is a string and not empty\n",
    "    if isinstance(text_1, str) and text_1.strip():\n",
    "        doc = {\n",
    "            \"id\": f\"{idx}_1\",\n",
    "            \"content\": text_1,\n",
    "            \"metadata\": {\"real\": row[\"real\"] == 1}\n",
    "        }\n",
    "        if row[\"real\"] == 1:\n",
    "            real_docs.append(doc)\n",
    "        else:\n",
    "            not_real_docs.append(doc)\n",
    "    # Only process if text_2 is a string and not empty\n",
    "    if isinstance(text_2, str) and text_2.strip():\n",
    "        doc = {\n",
    "            \"id\": f\"{idx}_2\",\n",
    "            \"content\": text_2,\n",
    "            \"metadata\": {\"real\": row[\"real\"] == 2}\n",
    "        }\n",
    "        if row[\"real\"] == 2:\n",
    "            real_docs.append(doc)\n",
    "        else:\n",
    "            not_real_docs.append(doc)\n",
    "\n",
    "# Delete the existing collection if it exists (to fix dimension mismatch)\n",
    "rebuild_collection = False\n",
    "if rebuild_collection:\n",
    "    vector_db_tmp = VectorDB(\n",
    "        collection_name=\"impostor_hunt_texts\",\n",
    "        embedding_length=384,\n",
    "        working_dir=os.getcwd()\n",
    "    )\n",
    "    vector_db_tmp.delete_collection()\n",
    "\n",
    "\n",
    "# Add late chunked documents for both groups\n",
    "vector_db_real = VectorDB(\n",
    "    collection_name=\"impostor_hunt_texts_real\",\n",
    "    embedding_length=768,\n",
    "    working_dir=os.getcwd(),\n",
    "    # embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "if rebuild_collection:\n",
    "    vector_db_real.add_documents_with_late_chunking(real_docs, chunk_size=1500, chunk_overlap=200, max_context=8192)\n",
    "    vector_db_real.add_documents_with_late_chunking(not_real_docs, chunk_size=1500, chunk_overlap=200, max_context=8192)\n",
    "\n",
    "search_limit = 20\n",
    "\n",
    "# count real/fake\n",
    "def count_real_fake(results, search_limit):\n",
    "    real_count = sum(1 for doc in results if doc['metadata']['real'])\n",
    "    fake_count = len(results) - real_count\n",
    "    return (real_count / search_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40f03771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embedding(text, device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(**inputs)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    return cls_emb\n",
    "\n",
    "def get_features_gpu_optimized(df, vector_db_real, search_limit=20, batch_size=8):\n",
    "    \"\"\"GPU-optimized feature extraction with batching\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Prepare all texts for batch processing\n",
    "    all_texts_1 = []\n",
    "    all_texts_2 = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        t1 = row['text_1']\n",
    "        t2 = row['text_2']\n",
    "        if isinstance(t1, str) and isinstance(t2, str):\n",
    "            all_texts_1.append(row['cleaned_text_1'])\n",
    "            all_texts_2.append(row['cleaned_text_2'])\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    print(f\"Processing {len(valid_indices)} valid text pairs...\")\n",
    "    \n",
    "    # Batch process embeddings for better GPU utilization\n",
    "    all_emb1 = []\n",
    "    all_emb2 = []\n",
    "    \n",
    "    # Process text_1 embeddings in batches\n",
    "    for i in tqdm(range(0, len(all_texts_1), batch_size), desc=\"Processing text_1 embeddings\"):\n",
    "        batch_texts = all_texts_1[i:i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if device.type == 'cuda':\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(**inputs)\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "            batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_emb1.extend(batch_emb)\n",
    "    \n",
    "    # Process text_2 embeddings in batches\n",
    "    for i in tqdm(range(0, len(all_texts_2), batch_size), desc=\"Processing text_2 embeddings\"):\n",
    "        batch_texts = all_texts_2[i:i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if device.type == 'cuda':\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(**inputs)\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "            batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_emb2.extend(batch_emb)\n",
    "    \n",
    "    # Clear GPU cache after batch processing\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Now process RAG scores and combine features\n",
    "    for i, idx in tqdm(enumerate(valid_indices), desc=\"Extracting RAG scores and combining features\"):\n",
    "        row = df.iloc[idx]\n",
    "        t1 = row['text_1']\n",
    "        t2 = row['text_2']\n",
    "        \n",
    "        emb1 = all_emb1[i]\n",
    "        emb2 = all_emb2[i]\n",
    "        \n",
    "        # Get RAG scores\n",
    "        score1 = count_real_fake(vector_db_real.search(t1, limit=search_limit), search_limit)\n",
    "        score2 = count_real_fake(vector_db_real.search(t2, limit=search_limit), search_limit)\n",
    "        \n",
    "        # Combine features\n",
    "        feat = np.concatenate([emb1, emb2, [score1, score2], emb1-emb2])\n",
    "        features.append(feat)\n",
    "        \n",
    "        if 'real' in row:\n",
    "            labels.append(1 if row['real'] == 1 else 2)\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Keep the original function as backup\n",
    "def get_features(df, vector_db_real, search_limit=20):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting features\"):\n",
    "        ct1 = row['cleaned_text_1']\n",
    "        ct2 = row['cleaned_text_2']\n",
    "        t1 = row['text_1']\n",
    "        t2 = row['text_2']\n",
    "        # Skip rows where t1 or t2 is not a string\n",
    "        if not isinstance(t1, str) or not isinstance(t2, str):\n",
    "            continue\n",
    "        emb1 = get_cls_embedding(ct1)\n",
    "        emb2 = get_cls_embedding(ct2)\n",
    "        score1 = count_real_fake(vector_db_real.search(t1, limit=search_limit), search_limit)\n",
    "        score2 = count_real_fake(vector_db_real.search(t2, limit=search_limit), search_limit)\n",
    "        feat = np.concatenate([emb1, emb2, [score1, score2], emb1-emb2])\n",
    "        features.append(feat)\n",
    "        if 'real' in row:\n",
    "            labels.append(1 if row['real'] == 1 else 2)\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30b71c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features:   0%|          | 0/95 [00:00<?, ?it/s]C:\\Users\\kabir\\AppData\\Local\\Temp\\ipykernel_106328\\3217240851.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Users\\kabir\\AppData\\Local\\Temp\\ipykernel_106328\\3217240851.py:10: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Extracting features: 100%|██████████| 95/95 [00:11<00:00,  8.46it/s]\n",
      "Extracting features: 100%|██████████| 95/95 [00:11<00:00,  8.46it/s]\n",
      "Extracting features: 100%|██████████| 1068/1068 [02:04<00:00,  8.61it/s]\n",
      "Extracting features: 100%|██████████| 1068/1068 [02:04<00:00,  8.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare train/test features ---\n",
    "X_train, y_train = get_features(paired_df, vector_db_real, search_limit=20)\n",
    "X_test, _ = get_features(test_df, vector_db_real, search_limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50a88002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BERT+RAG Prediction: 100%|██████████| 1068/1068 [01:09<00:00, 15.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'bert_rag_ensemble_predictions.csv'\n",
      "   id  real_text_id\n",
      "0   0             2\n",
      "1   1             2\n",
      "2   2             1\n",
      "3   3             1\n",
      "4   4             2\n",
      "Peak GPU Memory: 1.1GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Fix BERTClassifier training issues\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "print(\"Setting up BERTClassifier training...\")\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    True BERT-based classifier that processes raw text through BERT\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=2, dropout=0.3, freeze_bert=False):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size * 2, num_classes)  # *2 for pair classification\n",
    "        \n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, input_ids_1, attention_mask_1, input_ids_2, attention_mask_2):\n",
    "        # Process both texts through BERT\n",
    "        outputs_1 = self.bert(input_ids=input_ids_1, attention_mask=attention_mask_1)\n",
    "        outputs_2 = self.bert(input_ids=input_ids_2, attention_mask=attention_mask_2)\n",
    "        \n",
    "        # Use CLS tokens (pooler_output)\n",
    "        pooled_1 = outputs_1.pooler_output\n",
    "        pooled_2 = outputs_2.pooler_output\n",
    "        \n",
    "        # Concatenate representations\n",
    "        combined = torch.cat([pooled_1, pooled_2], dim=1)\n",
    "        combined = self.dropout(combined)\n",
    "        \n",
    "        return self.classifier(combined)\n",
    "\n",
    "class TextPairDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for text pair classification\"\"\"\n",
    "    def __init__(self, texts_1, texts_2, labels, tokenizer, max_length=128):\n",
    "        self.texts_1 = texts_1\n",
    "        self.texts_2 = texts_2\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts_1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text_1 = str(self.texts_1[idx])\n",
    "        text_2 = str(self.texts_2[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        tokens_1 = self.tokenizer(text_1, max_length=self.max_length, \n",
    "                                 truncation=True, padding='max_length', return_tensors='pt')\n",
    "        tokens_2 = self.tokenizer(text_2, max_length=self.max_length, \n",
    "                                 truncation=True, padding='max_length', return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids_1': tokens_1['input_ids'].squeeze(),\n",
    "            'attention_mask_1': tokens_1['attention_mask'].squeeze(),\n",
    "            'input_ids_2': tokens_2['input_ids'].squeeze(),\n",
    "            'attention_mask_2': tokens_2['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def train_bert_classifier_fixed(train_df, device, num_epochs=2, batch_size=8, max_length=128):\n",
    "    \"\"\"Fixed BERT classifier training\"\"\"\n",
    "    \n",
    "    print(\"Preparing training data...\")\n",
    "    \n",
    "    # Prepare training data\n",
    "    texts_1, texts_2, labels = [], [], []\n",
    "    for _, row in train_df.iterrows():\n",
    "        if pd.notna(row['cleaned_text_1']) and pd.notna(row['cleaned_text_2']):\n",
    "            texts_1.append(str(row['cleaned_text_1'])[:500])  # Truncate for safety\n",
    "            texts_2.append(str(row['cleaned_text_2'])[:500])  # Truncate for safety\n",
    "            # Convert labels: if real == 1, then text_1 is real (label 0), else text_2 is real (label 1)\n",
    "            labels.append(0 if row['real'] == 1 else 1)\n",
    "    \n",
    "    print(f\"Prepared {len(texts_1)} training samples\")\n",
    "    \n",
    "    # Use existing tokenizer to avoid loading another model\n",
    "    bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Initialize model with smaller batch size to avoid memory issues\n",
    "    print(\"Initializing BERTClassifier...\")\n",
    "    bert_classifier = BERTClassifier(num_classes=2, dropout=0.3, freeze_bert=True).to(device)  # Freeze BERT for faster training\n",
    "    \n",
    "    # Prepare data loader\n",
    "    dataset = TextPairDataset(texts_1, texts_2, labels, bert_tokenizer, max_length)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = optim.AdamW(bert_classifier.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(f\"Starting training for {num_epochs} epochs with batch size {batch_size}...\")\n",
    "    \n",
    "    # Training loop with debugging\n",
    "    bert_classifier.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Training\")):\n",
    "            try:\n",
    "                # Move batch to device\n",
    "                input_ids_1 = batch['input_ids_1'].to(device)\n",
    "                attention_mask_1 = batch['attention_mask_1'].to(device)\n",
    "                input_ids_2 = batch['input_ids_2'].to(device)\n",
    "                attention_mask_2 = batch['attention_mask_2'].to(device)\n",
    "                labels_batch = batch['labels'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                optimizer.zero_grad()\n",
    "                outputs = bert_classifier(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Print progress every 10 batches\n",
    "                if batch_idx % 10 == 0:\n",
    "                    print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "                    \n",
    "                # Clear cache periodically\n",
    "                if batch_idx % 20 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "        print(f'Epoch {epoch+1} completed - Average Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "    return bert_classifier, bert_tokenizer\n",
    "\n",
    "# Fixed RAG scorer to work with your vector_db\n",
    "class RAGScorer:\n",
    "    \"\"\"Fixed RAG-based scoring system\"\"\"\n",
    "    def __init__(self, vector_db_real):\n",
    "        self.vector_db_real = vector_db_real\n",
    "    \n",
    "    def get_scores(self, text_1, text_2, search_limit=20):\n",
    "        \"\"\"Get RAG scores for text pair\"\"\"\n",
    "        score1 = self.count_real_fake_fixed(self.vector_db_real.search(text_1, limit=search_limit), search_limit)\n",
    "        score2 = self.count_real_fake_fixed(self.vector_db_real.search(text_2, limit=search_limit), search_limit)\n",
    "        return score1, score2\n",
    "    \n",
    "    def count_real_fake_fixed(self, search_results, search_limit):\n",
    "        \"\"\"Fixed count real vs fake in search results\"\"\"\n",
    "        if not search_results:\n",
    "            return 0.5\n",
    "        \n",
    "        # Use the existing count_real_fake function that works with your vector_db\n",
    "        return count_real_fake(search_results, search_limit)\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU Memory before training: {torch.cuda.memory_allocated()/1024**3:.1f}GB\")\n",
    "\n",
    "# Train the BERTClassifier\n",
    "print(\"Training BERTClassifier...\")\n",
    "trained_bert_classifier, bert_tokenizer = train_bert_classifier_fixed(paired_df, device, num_epochs=100, batch_size=4)\n",
    "\n",
    "# Initialize RAG scorer\n",
    "print(\"Initializing RAG scorer...\")\n",
    "rag_scorer = RAGScorer(vector_db_real)\n",
    "\n",
    "# Simple ensemble prediction function\n",
    "def bert_rag_ensemble_predict(test_df, bert_model, rag_scorer, tokenizer, device, alpha=0.6, search_limit=20, max_length=128):\n",
    "    \"\"\"Ensemble prediction with BERT + RAG\"\"\"\n",
    "    results = []\n",
    "    bert_model.eval()\n",
    "    \n",
    "    print(f\"Making predictions on {len(test_df)} samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"BERT+RAG Prediction\"):\n",
    "            try:\n",
    "                text_1 = str(row.get('cleaned_text_1', ''))[:500]\n",
    "                text_2 = str(row.get('cleaned_text_2', ''))[:500]\n",
    "                \n",
    "                # BERT prediction\n",
    "                tokens_1 = tokenizer(text_1, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "                tokens_2 = tokenizer(text_2, max_length=max_length, truncation=True, padding='max_length', return_tensors='pt')\n",
    "                \n",
    "                input_ids_1 = tokens_1['input_ids'].to(device)\n",
    "                attention_mask_1 = tokens_1['attention_mask'].to(device)\n",
    "                input_ids_2 = tokens_2['input_ids'].to(device)\n",
    "                attention_mask_2 = tokens_2['attention_mask'].to(device)\n",
    "                \n",
    "                outputs = bert_model(input_ids_1, attention_mask_1, input_ids_2, attention_mask_2)\n",
    "                bert_probs = torch.softmax(outputs, dim=1)[0]\n",
    "                \n",
    "                # RAG prediction\n",
    "                rag_score1, rag_score2 = rag_scorer.get_scores(text_1, text_2, search_limit)\n",
    "                \n",
    "                # Ensemble combination\n",
    "                combined_score_1 = alpha * bert_probs[0] + (1-alpha) * rag_score1\n",
    "                combined_score_2 = alpha * bert_probs[1] + (1-alpha) * rag_score2\n",
    "                \n",
    "                predicted_real = 1 if combined_score_1 >= combined_score_2 else 2\n",
    "                \n",
    "                results.append({'id': idx, 'real_text_id': predicted_real})\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {idx}: {e}\")\n",
    "                # Fallback to RAG-only prediction\n",
    "                try:\n",
    "                    rag_score1, rag_score2 = rag_scorer.get_scores(text_1, text_2, search_limit)\n",
    "                    predicted_real = 1 if rag_score1 >= rag_score2 else 2\n",
    "                    results.append({'id': idx, 'real_text_id': predicted_real})\n",
    "                except:\n",
    "                    results.append({'id': idx, 'real_text_id': 1})  # Default prediction\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Make predictions\n",
    "print(\"Generating ensemble predictions...\")\n",
    "final_predictions = bert_rag_ensemble_predict(\n",
    "    test_df, \n",
    "    trained_bert_classifier, \n",
    "    rag_scorer, \n",
    "    bert_tokenizer, \n",
    "    device, \n",
    "    alpha=0.6\n",
    ")\n",
    "\n",
    "# Save results\n",
    "final_predictions.to_csv(\"bert_rag_ensemble_predictions.csv\", index=False)\n",
    "print(\"Predictions saved to 'bert_rag_ensemble_predictions.csv'\")\n",
    "print(final_predictions.head())\n",
    "\n",
    "# Monitor GPU usage\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Peak GPU Memory: {torch.cuda.max_memory_allocated()/1024**3:.1f}GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_real_fake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
