{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cb9a45e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.5-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from evaluate) (4.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from evaluate) (2.1.3)\n",
      "Requirement already satisfied: dill in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from evaluate) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from evaluate) (0.34.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=4.2 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from aiosignal>=1.4.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (4.14.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.8.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vishn\\onedrive\\desktop\\kaggle-impostor-hunt-texts\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading evaluate-0.4.5-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -r requirements.txt.12 scikit-learn\n",
    "#%pip install datasets\n",
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7b3bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utils.vector_db import VectorDB\n",
    "from chromadb import EmbeddingFunction\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import nlpaug.augmenter.word as naw\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    DebertaForSequenceClassification,\n",
    "    DebertaTokenizer,\n",
    "    AutoTokenizer\n",
    ")\n",
    "import evaluate\n",
    "from datasets import Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "389cd40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>real_text_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  real_text_id\n",
       "0   0             1\n",
       "1   1             2\n",
       "2   2             1\n",
       "3   3             2\n",
       "4   4             2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"data/train.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ef0237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the disney movie was absolutely excellent!']\n"
     ]
    }
   ],
   "source": [
    "aug=naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")\n",
    "print(aug.augment(\"The movie was excellent!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7964564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, using CPU\n",
      "Model loaded on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Available GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer with GPU support\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Enable mixed precision for faster training (if GPU supports it)\n",
    "if torch.cuda.is_available():\n",
    "    model.half()  # Use FP16 for faster inference\n",
    "    print(\"Model loaded with FP16 precision for faster GPU processing\")\n",
    "else:\n",
    "    print(\"GPU not available, using CPU\")\n",
    "\n",
    "print(f\"Model loaded on: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40d065a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a429cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c40be97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_or_create_paired_df(data_dir, csv_path, has_real=True, do_augment=True, n_Aug=10):\n",
    "    \"\"\"\n",
    "    If csv_path exists -> load it.\n",
    "    Else -> loop through article_* folders in data_dir and build a dataframe with:\n",
    "    - text_1, text_2\n",
    "    - real (only if has_real=True), looked up from <parent_of_data_dir>/train.csv\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        return pd.read_csv(csv_path)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    if has_real:\n",
    "        # load the csv at \"data/train.csv\"\n",
    "        real_df = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "    for article_dir in sorted(d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))):\n",
    "        article_path = os.path.join(data_dir, article_dir)\n",
    "        f1 = os.path.join(article_path, \"file_1.txt\")\n",
    "        f2 = os.path.join(article_path, \"file_2.txt\")\n",
    "\n",
    "        text_1 = read_text(f1)\n",
    "        text_2 = read_text(f2)\n",
    "\n",
    "        row = {\"text_1\": text_1, \"text_2\": text_2}\n",
    "\n",
    "        if has_real:\n",
    "            # lookup the \"real\" value from the real_df\n",
    "            real_row = real_df[real_df[\"id\"] == int(article_dir.split(\"_\")[1])]\n",
    "            real_value = real_row[\"real_text_id\"].values[0] if not real_row.empty else np.nan\n",
    "            row[\"real\"] = real_value\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "\n",
    "    if not do_augment:\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        return df\n",
    "\n",
    "    aug=naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"insert\")\n",
    "\n",
    "    augmented_df=[]\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        for _ in range(n_Aug):\n",
    "            new_text=aug.augment(row['text_1'], n=1)\n",
    "            new_text2=aug.augment(row['text_2'], n=1)\n",
    "            augmented_df.append({'text_1': new_text, 'text_2': new_text2, \"real\": row['real']})\n",
    "\n",
    "    aug_df = pd.DataFrame(augmented_df)\n",
    "    final_df = pd.concat([df, aug_df], ignore_index=True)\n",
    "    final_df.to_csv(csv_path.split(\".\")[0] + \"_augmented.csv\", index=False)\n",
    "    print(\"did but no resut\")\n",
    "\n",
    "    return final_df\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# Usage\n",
    "train_data_dir = \"data/train\"\n",
    "test_data_dir  = \"data/test\"\n",
    "train_csv = \"data/stored_train_data_augmented_augmented.csv\"\n",
    "test_csv  = \"data/stored_test_data.csv\"\n",
    "\n",
    "paired_df = load_or_create_paired_df(train_data_dir, train_csv, has_real=True, do_augment=True, n_Aug=10)\n",
    "test_df   = load_or_create_paired_df(test_data_dir,  test_csv,  has_real=False, do_augment=False, n_Aug=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa0dbcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "491dff92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "real\n",
       "2    539\n",
       "1    506\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paired_df['real'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d49d9115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>real</th>\n",
       "      <th>cleaned_text_1</th>\n",
       "      <th>cleaned_text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n",
       "      <td>The China relay network has released a signifi...</td>\n",
       "      <td>1</td>\n",
       "      <td>virsa visible infrared survey telescope array ...</td>\n",
       "      <td>china relay network released significant amoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China\\nThe goal of this project involves achie...</td>\n",
       "      <td>The project aims to achieve an accuracy level ...</td>\n",
       "      <td>2</td>\n",
       "      <td>china goal project involves achieving accuracy...</td>\n",
       "      <td>project aim achieve accuracy level dex analyzi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scientists can learn about how galaxies form a...</td>\n",
       "      <td>Dinosaur eggshells offer clues about what dino...</td>\n",
       "      <td>1</td>\n",
       "      <td>scientist learn galaxy form evolve two method ...</td>\n",
       "      <td>dinosaur eggshell offer clue dinosaur ate long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>China\\nThe study suggests that multiple star s...</td>\n",
       "      <td>The importance for understanding how stars evo...</td>\n",
       "      <td>2</td>\n",
       "      <td>china study suggests multiple star system play...</td>\n",
       "      <td>importance understanding star evolve led resea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dinosaur Rex was excited about his new toy set...</td>\n",
       "      <td>Analyzing how fast stars rotate within a galax...</td>\n",
       "      <td>2</td>\n",
       "      <td>dinosaur rex excited new toy set many dinosaur...</td>\n",
       "      <td>analyzing fast star rotate within galaxy compa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  The VIRSA (Visible Infrared Survey Telescope A...   \n",
       "1  China\\nThe goal of this project involves achie...   \n",
       "2  Scientists can learn about how galaxies form a...   \n",
       "3  China\\nThe study suggests that multiple star s...   \n",
       "4  Dinosaur Rex was excited about his new toy set...   \n",
       "\n",
       "                                              text_2  real  \\\n",
       "0  The China relay network has released a signifi...     1   \n",
       "1  The project aims to achieve an accuracy level ...     2   \n",
       "2  Dinosaur eggshells offer clues about what dino...     1   \n",
       "3  The importance for understanding how stars evo...     2   \n",
       "4  Analyzing how fast stars rotate within a galax...     2   \n",
       "\n",
       "                                      cleaned_text_1  \\\n",
       "0  virsa visible infrared survey telescope array ...   \n",
       "1  china goal project involves achieving accuracy...   \n",
       "2  scientist learn galaxy form evolve two method ...   \n",
       "3  china study suggests multiple star system play...   \n",
       "4  dinosaur rex excited new toy set many dinosaur...   \n",
       "\n",
       "                                      cleaned_text_2  \n",
       "0  china relay network released significant amoun...  \n",
       "1  project aim achieve accuracy level dex analyzi...  \n",
       "2  dinosaur eggshell offer clue dinosaur ate long...  \n",
       "3  importance understanding star evolve led resea...  \n",
       "4  analyzing fast star rotate within galaxy compa...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Join the tokens back into a cleaned string\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def clean_df(df):\n",
    "    df['cleaned_text_1'] = df['text_1'].apply(clean_text)\n",
    "    df['cleaned_text_2'] = df['text_2'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "paired_df = clean_df(paired_df)\n",
    "\n",
    "paired_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a7904e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = clean_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "abcc0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bert_embeddings(text, device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    # Tokenize input text\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.amp.autocast():  # Use automatic mixed precision\n",
    "                outputs = model(**inputs)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "        # The last hidden state contains the embeddings\n",
    "        embeddings = outputs.last_hidden_state.cpu()  # Move back to CPU for return\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self, model, tokenizer, device=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device if device is not None else next(model.parameters()).device\n",
    "\n",
    "    def __call__(self, input: list) -> list:\n",
    "        # input: list of strings\n",
    "        embeddings = []\n",
    "        \n",
    "        # Process in batches for better GPU utilization\n",
    "        batch_size = 16 if self.device.type == 'cuda' else 4\n",
    "        \n",
    "        for i in range(0, len(input), batch_size):\n",
    "            batch_texts = input[i:i + batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts, \n",
    "                return_tensors='pt', \n",
    "                truncation=True, \n",
    "                padding=True, \n",
    "                max_length=512\n",
    "            )\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if self.device.type == 'cuda':\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(**inputs)\n",
    "                else:\n",
    "                    outputs = self.model(**inputs)\n",
    "                \n",
    "                # Use the [CLS] token embedding as sentence embedding\n",
    "                cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                \n",
    "                for emb in cls_embeddings:\n",
    "                    embeddings.append(emb.tolist())\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Test the GPU-accelerated embedding function\n",
    "\n",
    "\n",
    "# Clear GPU cache if using CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba716b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = []\n",
    "# for idx, row in paired_df.iterrows():\n",
    "#     if str(row['cleaned_text_1']).strip():\n",
    "#         documents.append({\n",
    "#             \"id\": f\"{idx}_1\",\n",
    "#             \"content\": row['cleaned_text_1'],\n",
    "#             \"metadata\": {\"real\": row[\"real\"] == 1}\n",
    "#         })\n",
    "#     if str(row['cleaned_text_2']).strip():\n",
    "#         documents.append({\n",
    "#             \"id\": f\"{idx}_2\",\n",
    "#             \"content\": row['cleaned_text_2'],\n",
    "#             \"metadata\": {\"real\": row[\"real\"] == 2}\n",
    "#         })\n",
    "\n",
    "# # Delete the existing collection if it exists (to fix dimension mismatch)\n",
    "# rebuild_collection = False\n",
    "# if rebuild_collection:\n",
    "#     vector_db_tmp = VectorDB(\n",
    "#         collection_name=\"impostor_hunt_texts\",\n",
    "#         embedding_length=384,\n",
    "#         working_dir=os.getcwd()\n",
    "#     )\n",
    "#     vector_db_tmp.delete_collection()\n",
    "\n",
    "# embedding_function = MyEmbeddingFunction(model, tokenizer)\n",
    "\n",
    "\n",
    "# # Initialize VectorDB (embedding_function can be left as None to use default)\n",
    "# vector_db = VectorDB(\n",
    "#     collection_name=\"impostor_hunt_texts\",\n",
    "#     embedding_length=768,\n",
    "#     working_dir=os.getcwd(),\n",
    "#     documents=documents,\n",
    "#     dont_add_if_collection_exist=not rebuild_collection\n",
    "# )\n",
    "\n",
    "# vector_db.search(\"\"\"ChromeDriver music player\n",
    "# This study focused on identifying any non-spherical shapes within specific types of celestial bodies (music music) using various techniques like comparing how they look from different directions and analyzing their changes in sound pressure vs time .\n",
    "# The extent to which these artists' images show evidence for an overall shape rather than individual tracks was found across multiple tracks:\n",
    "# Two specific songs had clearly visible distortions due to their complex structure compared to others playing just simple beats\n",
    "# This research found that while most recordings showed a relatively simple structure (like when you only see one instrument rather than an entire grand orchestra), some featured noticeable deviations from those expectations (like if there were multiple instruments playing at once). These results suggest there may be a correlation between how musicians program their compositions and how much curvature they chose for their soundscape — it seems as though tracks with more intricate arrangements tend towards greater complexity!\n",
    "# Please note: This is just an example response based on your input text as I am not able access real world information such as music information or even what \"music music\" means without further context!\n",
    "# Let me know if you want me to try working through some real world examples instead? I can also provide alternative ways I could rephrase your initial statement!\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cca449a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Late Chunking for 'real' and 'not real' groups ---\n",
    "real_docs = []\n",
    "not_real_docs = []\n",
    "for idx, row in paired_df.iterrows():\n",
    "    text_1 = row['cleaned_text_1']\n",
    "    text_2 = row['cleaned_text_2']\n",
    "    # Only process if text_1 is a string and not empty\n",
    "    if isinstance(text_1, str) and text_1.strip():\n",
    "        doc = {\n",
    "            \"id\": f\"{idx}_1\",\n",
    "            \"content\": text_1,\n",
    "            \"metadata\": {\"real\": row[\"real\"] == 1}\n",
    "        }\n",
    "        if row[\"real\"] == 1:\n",
    "            real_docs.append(doc)\n",
    "        else:\n",
    "            not_real_docs.append(doc)\n",
    "    # Only process if text_2 is a string and not empty\n",
    "    if isinstance(text_2, str) and text_2.strip():\n",
    "        doc = {\n",
    "            \"id\": f\"{idx}_2\",\n",
    "            \"content\": text_2,\n",
    "            \"metadata\": {\"real\": row[\"real\"] == 2}\n",
    "        }\n",
    "        if row[\"real\"] == 2:\n",
    "            real_docs.append(doc)\n",
    "        else:\n",
    "            not_real_docs.append(doc)\n",
    "\n",
    "# Delete the existing collection if it exists (to fix dimension mismatch)\n",
    "rebuild_collection = False\n",
    "if rebuild_collection:\n",
    "    vector_db_tmp = VectorDB(\n",
    "        collection_name=\"impostor_hunt_texts\",\n",
    "        embedding_length=384,\n",
    "        working_dir=os.getcwd()\n",
    "    )\n",
    "    vector_db_tmp.delete_collection()\n",
    "\n",
    "\n",
    "# Add late chunked documents for both groups\n",
    "vector_db_real = VectorDB(\n",
    "    collection_name=\"impostor_hunt_texts_real\",\n",
    "    embedding_length=768,\n",
    "    working_dir=os.getcwd(),\n",
    "    # embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "if rebuild_collection:\n",
    "    vector_db_real.add_documents_with_late_chunking(real_docs, chunk_size=1500, chunk_overlap=200, max_context=8192)\n",
    "    vector_db_real.add_documents_with_late_chunking(not_real_docs, chunk_size=1500, chunk_overlap=200, max_context=8192)\n",
    "\n",
    "search_limit = 20\n",
    "\n",
    "# count real/fake\n",
    "def count_real_fake(results, search_limit):\n",
    "    real_count = sum(1 for doc in results if doc['metadata']['real'])\n",
    "    fake_count = len(results) - real_count\n",
    "    return (real_count / search_limit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde155a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "40f03771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cls_embedding(text, device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if device.type == 'cuda':\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(**inputs)\n",
    "        else:\n",
    "            outputs = model(**inputs)\n",
    "        cls_emb = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    return cls_emb\n",
    "\n",
    "def get_features_gpu_optimized(df, vector_db_real, search_limit=20, batch_size=8):\n",
    "    \"\"\"GPU-optimized feature extraction with batching\"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    # Prepare all texts for batch processing\n",
    "    all_texts_1 = []\n",
    "    all_texts_2 = []\n",
    "    valid_indices = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        t1 = row['text_1']\n",
    "        t2 = row['text_2']\n",
    "        if isinstance(t1, str) and isinstance(t2, str):\n",
    "            all_texts_1.append(row['cleaned_text_1'])\n",
    "            all_texts_2.append(row['cleaned_text_2'])\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    print(f\"Processing {len(valid_indices)} valid text pairs...\")\n",
    "    \n",
    "    # Batch process embeddings for better GPU utilization\n",
    "    all_emb1 = []\n",
    "    all_emb2 = []\n",
    "    \n",
    "    # Process text_1 embeddings in batches\n",
    "    for i in tqdm(range(0, len(all_texts_1), batch_size), desc=\"Processing text_1 embeddings\"):\n",
    "        batch_texts = all_texts_1[i:i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if device.type == 'cuda':\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(**inputs)\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "            batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_emb1.extend(batch_emb)\n",
    "    \n",
    "    # Process text_2 embeddings in batches\n",
    "    for i in tqdm(range(0, len(all_texts_2), batch_size), desc=\"Processing text_2 embeddings\"):\n",
    "        batch_texts = all_texts_2[i:i + batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch_texts, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if device.type == 'cuda':\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(**inputs)\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "            batch_emb = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "            all_emb2.extend(batch_emb)\n",
    "    \n",
    "    # Clear GPU cache after batch processing\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Now process RAG scores and combine features\n",
    "    for i, idx in tqdm(enumerate(valid_indices), desc=\"Extracting RAG scores and combining features\"):\n",
    "        row = df.iloc[idx]\n",
    "        t1 = row['text_1']\n",
    "        t2 = row['text_2']\n",
    "        \n",
    "        emb1 = all_emb1[i]\n",
    "        emb2 = all_emb2[i]\n",
    "        \n",
    "        # Get RAG scores\n",
    "        score1 = count_real_fake(vector_db_real.search(t1, limit=search_limit), search_limit)\n",
    "        score2 = count_real_fake(vector_db_real.search(t2, limit=search_limit), search_limit)\n",
    "        \n",
    "        # Combine features\n",
    "        feat = np.concatenate([emb1, emb2, [score1, score2], emb1-emb2])\n",
    "        features.append(feat)\n",
    "        \n",
    "        if 'real' in row:\n",
    "            labels.append(1 if row['real'] == 1 else 2)\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Keep the original function as backup\n",
    "def get_features(df, vector_db_real, search_limit=20):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Extracting features\"):\n",
    "        ct1 = row['cleaned_text_1']\n",
    "        ct2 = row['cleaned_text_2']\n",
    "        t1 = row['text_1']\n",
    "        t2 = row['text_2']\n",
    "        # Skip rows where t1 or t2 is not a string\n",
    "        if not isinstance(t1, str) or not isinstance(t2, str):\n",
    "            continue\n",
    "        emb1 = get_cls_embedding(ct1)\n",
    "        emb2 = get_cls_embedding(ct2)\n",
    "        score1 = count_real_fake(vector_db_real.search(t1, limit=search_limit), search_limit)\n",
    "        score2 = count_real_fake(vector_db_real.search(t2, limit=search_limit), search_limit)\n",
    "        feat = np.concatenate([emb1, emb2, [score1, score2], emb1-emb2])\n",
    "        features.append(feat)\n",
    "        if 'real' in row:\n",
    "            labels.append(1 if row['real'] == 1 else 2)\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "30b71c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 1045/1045 [10:46<00:00,  1.62it/s]\n",
      "Extracting features: 100%|██████████| 1045/1045 [10:46<00:00,  1.62it/s]\n",
      "Extracting features: 100%|██████████| 1068/1068 [08:36<00:00,  2.07it/s]\n",
      "Extracting features: 100%|██████████| 1068/1068 [08:36<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare train/test features ---\n",
    "X_train, y_train = get_features(paired_df, vector_db_real, search_limit=20)\n",
    "X_test, _ = get_features(test_df, vector_db_real, search_limit=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc3a81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f9947bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2306,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50a88002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Rename to better reflect what this classifier actually does\n",
    "# class FeatureClassifier(torch.nn.Module):\n",
    "#     \"\"\"\n",
    "#     Multi-layer neural network for classification using pre-extracted features:\n",
    "#     - BERT embeddings (768 * 2 = 1536 dims)\n",
    "#     - RAG similarity scores (2 dims) \n",
    "#     - Embedding differences (768 dims)\n",
    "#     Total input size: 2306 features\n",
    "#     \"\"\"\n",
    "#     def __init__(self, input_size, hidden_size=512, num_classes=2, dropout=0.3):\n",
    "#         super(FeatureClassifier, self).__init__()\n",
    "#         self.fc1 = torch.nn.Linear(input_size, hidden_size)\n",
    "#         self.fc2 = torch.nn.Linear(hidden_size, hidden_size // 2)\n",
    "#         self.fc3 = torch.nn.Linear(hidden_size // 2, num_classes)\n",
    "#         self.dropout = torch.nn.Dropout(dropout)\n",
    "#         self.relu = torch.nn.ReLU()\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.relu(self.fc2(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc3(x)\n",
    "#         return x\n",
    "\n",
    "# # Initialize the feature classifier\n",
    "# input_size = X_train.shape[1]  # Should be 2306\n",
    "# print(f\"Input feature size: {input_size}\")\n",
    "# feature_classifier = FeatureClassifier(input_size, hidden_size=512, num_classes=2, dropout=0.3).to(device)\n",
    "\n",
    "# # Training setup\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# # Convert labels to binary (0, 1) for binary classification\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train_encoded = label_encoder.fit_transform(y_train - 1)  # Convert 1,2 to 0,1\n",
    "\n",
    "# # Convert to tensors\n",
    "# X_train_tensor = torch.FloatTensor(X_train)\n",
    "# y_train_tensor = torch.LongTensor(y_train_encoded)\n",
    "# X_test_tensor = torch.FloatTensor(X_test)\n",
    "\n",
    "# # Create datasets and dataloaders\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# # Training parameters\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(feature_classifier.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 50\n",
    "# best_loss = float('inf')\n",
    "# patience = 10\n",
    "# patience_counter = 0\n",
    "\n",
    "# print(f\"Training feature classifier on {len(X_train)} samples...\")\n",
    "# feature_classifier.train()\n",
    "# for epoch in range(num_epochs):\n",
    "#     total_loss = 0\n",
    "#     for batch_x, batch_y in train_loader:\n",
    "#         batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = feature_classifier(batch_x)\n",
    "#         loss = criterion(outputs, batch_y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         total_loss += loss.item()\n",
    "    \n",
    "#     avg_loss = total_loss / len(train_loader)\n",
    "#     scheduler.step(avg_loss)\n",
    "    \n",
    "#     if (epoch + 1) % 10 == 0:\n",
    "#         print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    \n",
    "#     # Early stopping\n",
    "#     if avg_loss < best_loss:\n",
    "#         best_loss = avg_loss\n",
    "#         patience_counter = 0\n",
    "#     else:\n",
    "#         patience_counter += 1\n",
    "#         if patience_counter >= patience:\n",
    "#             print(f\"Early stopping at epoch {epoch+1}\")\n",
    "#             break\n",
    "\n",
    "# # Evaluate on test set\n",
    "# feature_classifier.eval()\n",
    "# with torch.no_grad():\n",
    "#     X_test_device = X_test_tensor.to(device)\n",
    "#     test_outputs = feature_classifier(X_test_device)\n",
    "#     test_probabilities = torch.softmax(test_outputs, dim=1)\n",
    "#     y_pred = torch.argmax(test_outputs, dim=1).cpu().numpy()\n",
    "\n",
    "# # Convert predictions back to original labels (1, 2)\n",
    "# y_pred_original = label_encoder.inverse_transform(y_pred) + 1\n",
    "\n",
    "# print(f\"Test predictions shape: {y_pred_original.shape}\")\n",
    "# print(f\"Unique predictions: {np.unique(y_pred_original)}\")\n",
    "\n",
    "# def ensemble_predict_features(test_df, vector_db_real, feature_classifier, X_test, device, search_limit=20, alpha=0.5):\n",
    "#     \"\"\"\n",
    "#     Enhanced ensemble prediction using the trained feature classifier.\n",
    "#     Combines classifier probabilities with RAG scores.\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "#     feature_classifier.eval()\n",
    "    \n",
    "#     # Get all classifier probabilities at once\n",
    "#     with torch.no_grad():\n",
    "#         X_test_tensor = torch.FloatTensor(X_test).to(device)\n",
    "#         classifier_probs = torch.softmax(feature_classifier(X_test_tensor), dim=1).cpu().numpy()\n",
    "    \n",
    "#     # Create a mapping from DataFrame index to prediction array index\n",
    "#     valid_indices = []\n",
    "#     for idx, row in test_df.iterrows():\n",
    "#         t1 = row['text_1']\n",
    "#         t2 = row['text_2']\n",
    "#         if isinstance(t1, str) and isinstance(t2, str):\n",
    "#             valid_indices.append(idx)\n",
    "    \n",
    "#     print(f\"Processing {len(valid_indices)} valid samples out of {len(test_df)} total samples\")\n",
    "#     print(f\"Classifier predictions array size: {classifier_probs.shape[0]}\")\n",
    "    \n",
    "#     # Create mapping from DataFrame index to prediction array index\n",
    "#     idx_to_pred_idx = {df_idx: pred_idx for pred_idx, df_idx in enumerate(valid_indices)}\n",
    "    \n",
    "#     for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Feature Ensemble\"):\n",
    "#         t1 = row['cleaned_text_1']\n",
    "#         t2 = row['cleaned_text_2']\n",
    "        \n",
    "#         # Check if this row has valid predictions\n",
    "#         if idx not in idx_to_pred_idx:\n",
    "#             # Handle invalid samples by using simple RAG-based prediction\n",
    "#             score1 = count_real_fake(vector_db_real.search(str(t1), limit=search_limit), search_limit)\n",
    "#             score2 = count_real_fake(vector_db_real.search(str(t2), limit=search_limit), search_limit)\n",
    "#             predicted_real = 1 if score1 >= score2 else 2\n",
    "#         else:\n",
    "#             # RAG scores\n",
    "#             score1 = count_real_fake(vector_db_real.search(t1, limit=search_limit), search_limit)\n",
    "#             score2 = count_real_fake(vector_db_real.search(t2, limit=search_limit), search_limit)\n",
    "            \n",
    "#             # Get classifier probabilities using the correct mapping\n",
    "#             pred_idx = idx_to_pred_idx[idx]\n",
    "#             classifier_prob = classifier_probs[pred_idx]\n",
    "            \n",
    "#             # Combine classifier predictions with RAG scores\n",
    "#             # classifier_prob[0] = probability that text_1 is real\n",
    "#             # classifier_prob[1] = probability that text_2 is real\n",
    "#             combined_1 = alpha * classifier_prob[0] + (1-alpha) * score1\n",
    "#             combined_2 = alpha * classifier_prob[1] + (1-alpha) * score2\n",
    "            \n",
    "#             predicted_real = 1 if combined_1 >= combined_2 else 2\n",
    "        \n",
    "#         results.append({'id': idx, 'real_text_id': predicted_real})\n",
    "    \n",
    "#     return pd.DataFrame(results)\n",
    "\n",
    "# # Generate ensemble predictions\n",
    "# ensemble_df = ensemble_predict_features(test_df, vector_db_real, feature_classifier, X_test, device, search_limit=20, alpha=0.5)\n",
    "# ensemble_df.to_csv(\"feature_ensemble_predictions.csv\", index=False)\n",
    "# print(\"Feature ensemble predictions saved!\")\n",
    "# ensemble_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12d59f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # --- GridSearchCV on the training split ---\n",
    "# param_grid = {\n",
    "#     'C': [0.1, 1, 10, 100],\n",
    "#     'kernel': ['linear', 'rbf', 'poly'],\n",
    "#     'gamma': ['scale', 'auto']\n",
    "# }\n",
    "\n",
    "# svm = SVC()\n",
    "# grid_search = GridSearchCV(\n",
    "#     svm, param_grid, cv=10, scoring='accuracy', verbose=1, n_jobs=-1\n",
    "# )\n",
    "\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print(\"\\nBest params:\", grid_search.best_params_)\n",
    "# print(\"Best CV accuracy: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "# # --- Final Model: Predict on Test Data (unlabeled) ---\n",
    "# y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# # Build DataFrame like your example\n",
    "# pred_df = pd.DataFrame({\n",
    "#     \"id\": range(len(y_pred)),\n",
    "#     \"real_text_id\": y_pred\n",
    "# })\n",
    "\n",
    "# # Save to CSV if needed\n",
    "# pred_df.to_csv(\"submission1.csv\", index=False)\n",
    "# print(pred_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99a7b1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, val_df=train_test_split(paired_df, test_size=0.2, random_state=42, stratify=paired_df['real'])\n",
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "537f8eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>real</th>\n",
       "      <th>cleaned_text_1</th>\n",
       "      <th>cleaned_text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>[dioramas is designed to observe faint astrono...</td>\n",
       "      <td>[visible and near infrared ( nir ) coverage ha...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>838</th>\n",
       "      <td>[the primary mirror design of the european ext...</td>\n",
       "      <td>[the primary mirror design for the european ex...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>[from the glittering expanse of the cosmos, ta...</td>\n",
       "      <td>[the only modification made to the system was ...</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>[# # cosmic alchemy : the vlt survey telescope...</td>\n",
       "      <td>[presently the turin vlt survey telescope ( vs...</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>[the * * wrybeasts * * in the * * o observator...</td>\n",
       "      <td>[the azimuth drive system experienced problems...</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text_1  \\\n",
       "305  [dioramas is designed to observe faint astrono...   \n",
       "838  [the primary mirror design of the european ext...   \n",
       "605  [from the glittering expanse of the cosmos, ta...   \n",
       "387  [# # cosmic alchemy : the vlt survey telescope...   \n",
       "423  [the * * wrybeasts * * in the * * o observator...   \n",
       "\n",
       "                                                text_2  real cleaned_text_1  \\\n",
       "305  [visible and near infrared ( nir ) coverage ha...     1                  \n",
       "838  [the primary mirror design for the european ex...     1                  \n",
       "605  [the only modification made to the system was ...     2                  \n",
       "387  [presently the turin vlt survey telescope ( vs...     2                  \n",
       "423  [the azimuth drive system experienced problems...     2                  \n",
       "\n",
       "    cleaned_text_2  \n",
       "305                 \n",
       "838                 \n",
       "605                 \n",
       "387                 \n",
       "423                 "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb597c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786d942e9f3c41a3ac608f3626eafc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vishn\\OneDrive\\Desktop\\kaggle-impostor-hunt-texts\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vishn\\.cache\\huggingface\\hub\\models--microsoft--deberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f8215ec0d24a34b80ee6e48e9ed6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 768)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n) argument after ** must be a mapping, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28minput\u001b[39m= X_train\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     logits=model(**\u001b[38;5;28minput\u001b[39m).logits\n\u001b[32m      8\u001b[39m predicted_class_id=logits.argmax().item()\n\u001b[32m      9\u001b[39m model.config.id2label[predicted_class_id]\n",
      "\u001b[31mTypeError\u001b[39m: DebertaForSequenceClassification(\n  (deberta): DebertaModel(\n    (embeddings): DebertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=0)\n      (LayerNorm): DebertaLayerNorm()\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): DebertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x DebertaLayer(\n          (attention): DebertaAttention(\n            (self): DisentangledSelfAttention(\n              (in_proj): Linear(in_features=768, out_features=2304, bias=False)\n              (pos_dropout): Dropout(p=0.1, inplace=False)\n              (pos_proj): Linear(in_features=768, out_features=768, bias=False)\n              (pos_q_proj): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): DebertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): DebertaLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): DebertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): DebertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): DebertaLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (rel_embeddings): Embedding(1024, 768)\n    )\n  )\n  (pooler): ContextPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n) argument after ** must be a mapping, not numpy.ndarray"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d7627c69eb47fd8059c97abdb55655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model=DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=2)\n",
    "\n",
    "input= X_train\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits=model(**input).logits\n",
    "\n",
    "predicted_class_id=logits.argmax().item()\n",
    "model.config.id2label[predicted_class_id]\n",
    "\n",
    "labels = torch.tensor([1])\n",
    "loss = model(**input, labels=labels).loss\n",
    "round(loss.item(), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0ce19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "# Example: use a sample from your paired_df for inference\n",
    "sample_text_1 = paired_df.iloc[0][\"cleaned_text_1\"] if \"cleaned_text_1\" in paired_df.columns else paired_df.iloc[0][\"text_1\"]\n",
    "inputs = tokenizer(sample_text_1, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "predicted_class_id = logits.argmax().item()\n",
    "print(f\"Predicted class: {model.config.id2label[predicted_class_id]}\")\n",
    "\n",
    "# Training: set num_labels to match your task\n",
    "num_labels = len(model.config.id2label)\n",
    "model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=num_labels)\n",
    "\n",
    "# Example label (replace with your actual label from paired_df)\n",
    "label = torch.tensor([paired_df.iloc[0][\"real\"]])\n",
    "loss = model(**inputs, labels=label).loss\n",
    "print(f\"Loss: {round(loss.item(), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67119c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 30086,    38,   524,   748,  1173, 18373,   328,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "inputs = tokenizer(\"Hi I am vishnu!\", return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79903d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d8ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61831fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
