{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9a45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b3bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utils.vector_db import VectorDB\n",
    "from chromadb import EmbeddingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7964564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40d065a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c40be97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "def load_or_create_paired_df(data_dir, csv_path, has_real=True):\n",
    "    \"\"\"\n",
    "    If csv_path exists -> load it.\n",
    "    Else -> loop through article_* folders in data_dir and build a dataframe with:\n",
    "      - text_1, text_2\n",
    "      - real (only if has_real=True), looked up from <parent_of_data_dir>/train.csv\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    if os.path.exists(csv_path):\n",
    "        return pd.read_csv(csv_path)\n",
    "\n",
    "    # Build mapping from article_id -> real (only when requested)\n",
    "    real_map = {}\n",
    "    if has_real:\n",
    "        train_csv_path = os.path.join(os.path.dirname(data_dir), \"train.csv\")\n",
    "        if os.path.exists(train_csv_path):\n",
    "            df_real = pd.read_csv(train_csv_path)\n",
    "            # Pick likely id/real columns with simple heuristics\n",
    "            id_col = next((c for c in df_real.columns if c.lower() in (\"article_id\", \"id\")), df_real.columns[0])\n",
    "            real_col = next((c for c in df_real.columns if c.lower() == \"real\"), None)\n",
    "\n",
    "            if real_col is not None:\n",
    "                def to_int_id(x):\n",
    "                    s = str(x)\n",
    "                    m = re.search(r\"\\d+\", s)\n",
    "                    return int(m.group()) if m else None\n",
    "\n",
    "                for _, r in df_real.iterrows():\n",
    "                    iid = to_int_id(r[id_col])\n",
    "                    if iid is not None:\n",
    "                        real_map[iid] = r[real_col]\n",
    "\n",
    "    def read_text(path):\n",
    "        try:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return f.read()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "    rows = []\n",
    "    for article_dir in sorted(d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))):\n",
    "        article_path = os.path.join(data_dir, article_dir)\n",
    "        f1 = os.path.join(article_path, \"file_1.txt\")\n",
    "        f2 = os.path.join(article_path, \"file_2.txt\")\n",
    "\n",
    "        text_1 = read_text(f1)\n",
    "        text_2 = read_text(f2)\n",
    "\n",
    "        row = {\"text_1\": text_1, \"text_2\": text_2}\n",
    "\n",
    "        if has_real:\n",
    "            # article_0001 -> 1\n",
    "            try:\n",
    "                iid = int(re.search(r\"\\d+\", article_dir).group())\n",
    "            except Exception:\n",
    "                iid = None\n",
    "            row[\"real\"] = real_map.get(iid, None)\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    paired_df = pd.DataFrame(rows)\n",
    "    paired_df.to_csv(csv_path, index=False)\n",
    "    return paired_df\n",
    "\n",
    "# Usage\n",
    "train_data_dir = \"data/train\"\n",
    "test_data_dir  = \"data/test\"\n",
    "train_csv = \"data/stored_train_data.csv\"\n",
    "test_csv  = \"data/stored_test_data.csv\"\n",
    "\n",
    "paired_df = load_or_create_paired_df(train_data_dir, train_csv, has_real=True)\n",
    "test_df   = load_or_create_paired_df(test_data_dir,  test_csv,  has_real=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d49d9115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>real</th>\n",
       "      <th>cleaned_text_1</th>\n",
       "      <th>cleaned_text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n",
       "      <td>The China relay network has released a signifi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>virsa visible infrared survey telescope array ...</td>\n",
       "      <td>china relay network released significant amoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China\\nThe goal of this project involves achie...</td>\n",
       "      <td>The project aims to achieve an accuracy level ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>china goal project involves achieving accuracy...</td>\n",
       "      <td>project aim achieve accuracy level dex analyzi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scientists can learn about how galaxies form a...</td>\n",
       "      <td>Dinosaur eggshells offer clues about what dino...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scientist learn galaxy form evolve two method ...</td>\n",
       "      <td>dinosaur eggshell offer clue dinosaur ate long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>China\\nThe study suggests that multiple star s...</td>\n",
       "      <td>The importance for understanding how stars evo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>china study suggests multiple star system play...</td>\n",
       "      <td>importance understanding star evolve led resea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dinosaur Rex was excited about his new toy set...</td>\n",
       "      <td>Analyzing how fast stars rotate within a galax...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dinosaur rex excited new toy set many dinosaur...</td>\n",
       "      <td>analyzing fast star rotate within galaxy compa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  The VIRSA (Visible Infrared Survey Telescope A...   \n",
       "1  China\\nThe goal of this project involves achie...   \n",
       "2  Scientists can learn about how galaxies form a...   \n",
       "3  China\\nThe study suggests that multiple star s...   \n",
       "4  Dinosaur Rex was excited about his new toy set...   \n",
       "\n",
       "                                              text_2  real  \\\n",
       "0  The China relay network has released a signifi...   NaN   \n",
       "1  The project aims to achieve an accuracy level ...   NaN   \n",
       "2  Dinosaur eggshells offer clues about what dino...   NaN   \n",
       "3  The importance for understanding how stars evo...   NaN   \n",
       "4  Analyzing how fast stars rotate within a galax...   NaN   \n",
       "\n",
       "                                      cleaned_text_1  \\\n",
       "0  virsa visible infrared survey telescope array ...   \n",
       "1  china goal project involves achieving accuracy...   \n",
       "2  scientist learn galaxy form evolve two method ...   \n",
       "3  china study suggests multiple star system play...   \n",
       "4  dinosaur rex excited new toy set many dinosaur...   \n",
       "\n",
       "                                      cleaned_text_2  \n",
       "0  china relay network released significant amoun...  \n",
       "1  project aim achieve accuracy level dex analyzi...  \n",
       "2  dinosaur eggshell offer clue dinosaur ate long...  \n",
       "3  importance understanding star evolve led resea...  \n",
       "4  analyzing fast star rotate within galaxy compa...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Join the tokens back into a cleaned string\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def clean_df(df):\n",
    "    df['cleaned_text_1'] = df['text_1'].apply(clean_text)\n",
    "    df['cleaned_text_2'] = df['text_2'].apply(clean_text)\n",
    "    return df\n",
    "\n",
    "paired_df = clean_df(paired_df)\n",
    "paired_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7904e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = clean_df(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abcc0197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_bert_embeddings(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # The last hidden state contains the embeddings\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "class MyEmbeddingFunction(EmbeddingFunction):\n",
    "    def __init__(self, model, tokenizer):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input: list) -> list:\n",
    "        # input: list of strings\n",
    "        embeddings = []\n",
    "        for text in input:\n",
    "            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "            # with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Use the [CLS] token embedding as sentence embedding\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().detach().cpu().numpy()\n",
    "            embeddings.append(cls_embedding.tolist())\n",
    "        return embeddings\n",
    "\n",
    "(extract_bert_embeddings(\"Sample text for embedding.\").shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dba716b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = []\n",
    "# for idx, row in paired_df.iterrows():\n",
    "#     if str(row['cleaned_text_1']).strip():\n",
    "#         documents.append({\n",
    "#             \"id\": f\"{idx}_1\",\n",
    "#             \"content\": row['cleaned_text_1'],\n",
    "#             \"metadata\": {\"real\": row[\"real\"] == 1}\n",
    "#         })\n",
    "#     if str(row['cleaned_text_2']).strip():\n",
    "#         documents.append({\n",
    "#             \"id\": f\"{idx}_2\",\n",
    "#             \"content\": row['cleaned_text_2'],\n",
    "#             \"metadata\": {\"real\": row[\"real\"] == 2}\n",
    "#         })\n",
    "\n",
    "# # Delete the existing collection if it exists (to fix dimension mismatch)\n",
    "# rebuild_collection = False\n",
    "# if rebuild_collection:\n",
    "#     vector_db_tmp = VectorDB(\n",
    "#         collection_name=\"impostor_hunt_texts\",\n",
    "#         embedding_length=384,\n",
    "#         working_dir=os.getcwd()\n",
    "#     )\n",
    "#     vector_db_tmp.delete_collection()\n",
    "\n",
    "# embedding_function = MyEmbeddingFunction(model, tokenizer)\n",
    "\n",
    "\n",
    "# # Initialize VectorDB (embedding_function can be left as None to use default)\n",
    "# vector_db = VectorDB(\n",
    "#     collection_name=\"impostor_hunt_texts\",\n",
    "#     embedding_length=768,\n",
    "#     working_dir=os.getcwd(),\n",
    "#     documents=documents,\n",
    "#     dont_add_if_collection_exist=not rebuild_collection\n",
    "# )\n",
    "\n",
    "# vector_db.search(\"\"\"ChromeDriver music player\n",
    "# This study focused on identifying any non-spherical shapes within specific types of celestial bodies (music music) using various techniques like comparing how they look from different directions and analyzing their changes in sound pressure vs time .\n",
    "# The extent to which these artists' images show evidence for an overall shape rather than individual tracks was found across multiple tracks:\n",
    "# Two specific songs had clearly visible distortions due to their complex structure compared to others playing just simple beats\n",
    "# This research found that while most recordings showed a relatively simple structure (like when you only see one instrument rather than an entire grand orchestra), some featured noticeable deviations from those expectations (like if there were multiple instruments playing at once). These results suggest there may be a correlation between how musicians program their compositions and how much curvature they chose for their soundscape — it seems as though tracks with more intricate arrangements tend towards greater complexity!\n",
    "# Please note: This is just an example response based on your input text as I am not able access real world information such as music information or even what \"music music\" means without further context!\n",
    "# Let me know if you want me to try working through some real world examples instead? I can also provide alternative ways I could rephrase your initial statement!\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cca449a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Late Chunking for 'real' and 'not real' groups ---\n",
    "real_docs = []\n",
    "not_real_docs = []\n",
    "for idx, row in paired_df.iterrows():\n",
    "    text_1 = row['cleaned_text_1']\n",
    "    text_2 = row['cleaned_text_2']\n",
    "    # Only process if text_1 is a string and not empty\n",
    "    if isinstance(text_1, str) and text_1.strip():\n",
    "        doc = {\n",
    "            \"id\": f\"{idx}_1\",\n",
    "            \"content\": text_1,\n",
    "            \"metadata\": {\"real\": row[\"real\"] == 1}\n",
    "        }\n",
    "        if row[\"real\"] == 1:\n",
    "            real_docs.append(doc)\n",
    "        else:\n",
    "            not_real_docs.append(doc)\n",
    "    # Only process if text_2 is a string and not empty\n",
    "    if isinstance(text_2, str) and text_2.strip():\n",
    "        doc = {\n",
    "            \"id\": f\"{idx}_2\",\n",
    "            \"content\": text_2,\n",
    "            \"metadata\": {\"real\": row[\"real\"] == 2}\n",
    "        }\n",
    "        if row[\"real\"] == 2:\n",
    "            real_docs.append(doc)\n",
    "        else:\n",
    "            not_real_docs.append(doc)\n",
    "\n",
    "# Delete the existing collection if it exists (to fix dimension mismatch)\n",
    "rebuild_collection = False\n",
    "if rebuild_collection:\n",
    "    vector_db_tmp = VectorDB(\n",
    "        collection_name=\"impostor_hunt_texts\",\n",
    "        embedding_length=384,\n",
    "        working_dir=os.getcwd()\n",
    "    )\n",
    "    vector_db_tmp.delete_collection()\n",
    "\n",
    "\n",
    "# Add late chunked documents for both groups\n",
    "vector_db_real = VectorDB(\n",
    "    collection_name=\"impostor_hunt_texts_real\",\n",
    "    embedding_length=768,\n",
    "    working_dir=os.getcwd(),\n",
    "    # embedding_function=embedding_function\n",
    ")\n",
    "\n",
    "if rebuild_collection:\n",
    "    vector_db_real.add_documents_with_late_chunking(real_docs, chunk_size=1500, chunk_overlap=200, max_context=8192)\n",
    "    vector_db_real.add_documents_with_late_chunking(not_real_docs, chunk_size=1500, chunk_overlap=200, max_context=8192)\n",
    "\n",
    "search_limit = 20\n",
    "\n",
    "# count real/fake\n",
    "def count_real_fake(results, search_limit):\n",
    "    real_count = sum(1 for doc in results if doc['metadata']['real'])\n",
    "    fake_count = len(results) - real_count\n",
    "    return (real_count / search_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10744a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying: 100%|██████████| 1068/1068 [00:57<00:00, 18.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>real_text_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1063</th>\n",
       "      <td>1063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064</th>\n",
       "      <td>1064</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1066</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1068 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  real_text_id\n",
       "0        0             2\n",
       "1        1             2\n",
       "2        2             1\n",
       "3        3             1\n",
       "4        4             2\n",
       "...    ...           ...\n",
       "1063  1063             1\n",
       "1064  1064             1\n",
       "1065  1065             1\n",
       "1066  1066             2\n",
       "1067  1067             1\n",
       "\n",
       "[1068 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classify_real_texts(paired_df, vector_db_real, search_limit=20):\n",
    "    results = []\n",
    "    for idx, row in tqdm(paired_df.iterrows(), total=len(paired_df), desc=\"Classifying\"):\n",
    "        text_1 = row['cleaned_text_1']\n",
    "        text_2 = row['cleaned_text_2']\n",
    "\n",
    "        score_1 = 0\n",
    "        score_2 = 0\n",
    "\n",
    "        if isinstance(text_1, str) and text_1.strip():\n",
    "            res_1 = vector_db_real.search(text_1, limit=search_limit)\n",
    "            score_1 = count_real_fake(res_1, search_limit)\n",
    "        if isinstance(text_2, str) and text_2.strip():\n",
    "            res_2 = vector_db_real.search(text_2, limit=search_limit)\n",
    "            score_2 = count_real_fake(res_2, search_limit)\n",
    "\n",
    "        predicted_real = 1 if score_1 >= score_2 else 2\n",
    "        results.append({'id': idx, 'real_text_id': predicted_real})\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Usage:\n",
    "predictions_df = classify_real_texts(test_df, vector_db_real, search_limit=20)\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc7bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_csv(\"predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle_real_fake",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
