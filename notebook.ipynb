{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb9a45e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.1.7-cp37-abi3-win_amd64.whl.metadata (703 bytes)\n",
      "Downloading hf_xet-1.1.7-cp37-abi3-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.6/2.8 MB 9.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 9.6 MB/s  0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.1.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install pandas nltk transformers torch\n",
    "#%pip install -r requirements.txt\n",
    "%pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7b3bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from utils.vector_db import VectorDB\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7964564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40d065a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\vishn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vishn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vishn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vishn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c40be97d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n",
       "      <td>The China relay network has released a signifi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China\\nThe goal of this project involves achie...</td>\n",
       "      <td>The project aims to achieve an accuracy level ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scientists can learn about how galaxies form a...</td>\n",
       "      <td>Dinosaur eggshells offer clues about what dino...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>China\\nThe study suggests that multiple star s...</td>\n",
       "      <td>The importance for understanding how stars evo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dinosaur Rex was excited about his new toy set...</td>\n",
       "      <td>Analyzing how fast stars rotate within a galax...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  The VIRSA (Visible Infrared Survey Telescope A...   \n",
       "1  China\\nThe goal of this project involves achie...   \n",
       "2  Scientists can learn about how galaxies form a...   \n",
       "3  China\\nThe study suggests that multiple star s...   \n",
       "4  Dinosaur Rex was excited about his new toy set...   \n",
       "\n",
       "                                              text_2  real  \n",
       "0  The China relay network has released a signifi...     1  \n",
       "1  The project aims to achieve an accuracy level ...     2  \n",
       "2  Dinosaur eggshells offer clues about what dino...     1  \n",
       "3  The importance for understanding how stars evo...     2  \n",
       "4  Analyzing how fast stars rotate within a galax...     2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data/\"\n",
    "train_data_dir = \"data/train/\"\n",
    "train_csv = \"data/train.csv\"\n",
    "\n",
    "stored_data_path = \"data/stored_data.csv\"\n",
    "\n",
    "if os.path.exists(stored_data_path):\n",
    "    # Load stored data\n",
    "    paired_df = pd.read_csv(stored_data_path)\n",
    "else:\n",
    "    # Load train.csv\n",
    "    df = pd.read_csv(train_csv)\n",
    "\n",
    "    # Detect correct column names\n",
    "    article_id_col = \"article_id\" if \"article_id\" in df.columns else df.columns[0]\n",
    "    real_col = \"real\" if \"real\" in df.columns else df.columns[-1]\n",
    "\n",
    "    # Prepare list for paired texts\n",
    "    paired_data = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        article_id = f\"article_{int(row[article_id_col]):04d}\"\n",
    "        file_1_path = os.path.join(train_data_dir, article_id, \"file_1.txt\")\n",
    "        file_2_path = os.path.join(train_data_dir, article_id, \"file_2.txt\")\n",
    "        try:\n",
    "            with open(file_1_path, \"r\", encoding=\"utf-8\") as f1:\n",
    "                text_1 = f1.read()\n",
    "        except Exception:\n",
    "            text_1 = \"\"\n",
    "        try:\n",
    "            with open(file_2_path, \"r\", encoding=\"utf-8\") as f2:\n",
    "                text_2 = f2.read()\n",
    "        except Exception:\n",
    "            text_2 = \"\"\n",
    "        paired_data.append({\n",
    "            \"text_1\": text_1,\n",
    "            \"text_2\": text_2,\n",
    "            \"real\": row[real_col]  # 1 or 2\n",
    "        })\n",
    "\n",
    "    paired_df = pd.DataFrame(paired_data)\n",
    "    paired_df.to_csv(stored_data_path, index=False)\n",
    "paired_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d49d9115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_1</th>\n",
       "      <th>text_2</th>\n",
       "      <th>real</th>\n",
       "      <th>cleaned_text_1</th>\n",
       "      <th>cleaned_text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n",
       "      <td>The China relay network has released a signifi...</td>\n",
       "      <td>1</td>\n",
       "      <td>virsa visible infrared survey telescope array ...</td>\n",
       "      <td>china relay network released significant amoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China\\nThe goal of this project involves achie...</td>\n",
       "      <td>The project aims to achieve an accuracy level ...</td>\n",
       "      <td>2</td>\n",
       "      <td>china goal project involves achieving accuracy...</td>\n",
       "      <td>project aim achieve accuracy level dex analyzi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scientists can learn about how galaxies form a...</td>\n",
       "      <td>Dinosaur eggshells offer clues about what dino...</td>\n",
       "      <td>1</td>\n",
       "      <td>scientist learn galaxy form evolve two method ...</td>\n",
       "      <td>dinosaur eggshell offer clue dinosaur ate long...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>China\\nThe study suggests that multiple star s...</td>\n",
       "      <td>The importance for understanding how stars evo...</td>\n",
       "      <td>2</td>\n",
       "      <td>china study suggests multiple star system play...</td>\n",
       "      <td>importance understanding star evolve led resea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dinosaur Rex was excited about his new toy set...</td>\n",
       "      <td>Analyzing how fast stars rotate within a galax...</td>\n",
       "      <td>2</td>\n",
       "      <td>dinosaur rex excited new toy set many dinosaur...</td>\n",
       "      <td>analyzing fast star rotate within galaxy compa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_1  \\\n",
       "0  The VIRSA (Visible Infrared Survey Telescope A...   \n",
       "1  China\\nThe goal of this project involves achie...   \n",
       "2  Scientists can learn about how galaxies form a...   \n",
       "3  China\\nThe study suggests that multiple star s...   \n",
       "4  Dinosaur Rex was excited about his new toy set...   \n",
       "\n",
       "                                              text_2  real  \\\n",
       "0  The China relay network has released a signifi...     1   \n",
       "1  The project aims to achieve an accuracy level ...     2   \n",
       "2  Dinosaur eggshells offer clues about what dino...     1   \n",
       "3  The importance for understanding how stars evo...     2   \n",
       "4  Analyzing how fast stars rotate within a galax...     2   \n",
       "\n",
       "                                      cleaned_text_1  \\\n",
       "0  virsa visible infrared survey telescope array ...   \n",
       "1  china goal project involves achieving accuracy...   \n",
       "2  scientist learn galaxy form evolve two method ...   \n",
       "3  china study suggests multiple star system play...   \n",
       "4  dinosaur rex excited new toy set many dinosaur...   \n",
       "\n",
       "                                      cleaned_text_2  \n",
       "0  china relay network released significant amoun...  \n",
       "1  project aim achieve accuracy level dex analyzi...  \n",
       "2  dinosaur eggshell offer clue dinosaur ate long...  \n",
       "3  importance understanding star evolve led resea...  \n",
       "4  analyzing fast star rotate within galaxy compa...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "    \n",
    "    # Join the tokens back into a cleaned string\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "\n",
    "paired_df['cleaned_text_1'] = paired_df['text_1'].apply(clean_text)\n",
    "paired_df['cleaned_text_2'] = paired_df['text_2'].apply(clean_text)\n",
    "paired_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abcc0197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bert_embeddings(text):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # The last hidden state contains the embeddings\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c2cecb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 95 rows for cleaned_text_1 and cleaned_text_2.\n"
     ]
    }
   ],
   "source": [
    "# Loop through both columns and extract embeddings\n",
    "embeddings_list_1 = []\n",
    "embeddings_list_2 = []\n",
    "\n",
    "for index, row in paired_df.iterrows():\n",
    "    # Extract embeddings for cleaned_text_1\n",
    "    sample_text_1 = row['cleaned_text_1']\n",
    "    embeddings_1 = extract_bert_embeddings(sample_text_1)\n",
    "    embeddings_list_1.append(embeddings_1)\n",
    "\n",
    "    # Extract embeddings for cleaned_text_2\n",
    "    sample_text_2 = row['cleaned_text_2']\n",
    "    embeddings_2 = extract_bert_embeddings(sample_text_2)\n",
    "    embeddings_list_2.append(embeddings_2)\n",
    "\n",
    "# Convert embeddings lists to tensors or save them as needed\n",
    "print(f\"Processed {len(embeddings_list_1)} rows for cleaned_text_1 and cleaned_text_2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c625e41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dba716b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'google_api_key' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     11\u001b[39m non_empty_documents = [doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents \u001b[38;5;28;01mif\u001b[39;00m doc.page_content.strip() != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Initialize VectorDB\u001b[39;00m\n\u001b[32m     14\u001b[39m vector_db = VectorDB(\n\u001b[32m     15\u001b[39m     collection_name=\u001b[33m\"\u001b[39m\u001b[33mimpostor_hunt_texts\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     embedding_length=\u001b[32m768\u001b[39m,  \u001b[38;5;66;03m# Gemini embedding size, adjust if needed\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     google_api_key=\u001b[43mgoogle_api_key\u001b[49m,\n\u001b[32m     18\u001b[39m     working_dir=os.getcwd(),\n\u001b[32m     19\u001b[39m     documents=non_empty_documents\n\u001b[32m     20\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'google_api_key' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "# Prepare documents for both columns\n",
    "documents = []\n",
    "for idx, row in paired_df.iterrows():\n",
    "    doc1 = Document(page_content=row['cleaned_text_1'], metadata={\"id\": f\"{idx}_1\", \"real\": row[\"real\"]})\n",
    "    doc2 = Document(page_content=row['cleaned_text_2'], metadata={\"id\": f\"{idx}_2\", \"real\": row[\"real\"]})\n",
    "    documents.extend([doc1, doc2])\n",
    "\n",
    "# Filter out documents with empty page_content\n",
    "non_empty_documents = [doc for doc in documents if doc.page_content.strip() != \"\"]\n",
    "\n",
    "# Initialize VectorDB\n",
    "vector_db = VectorDB(\n",
    "    collection_name=\"impostor_hunt_texts\",\n",
    "    embedding_length=768,  # Gemini embedding size, adjust if needed\n",
    "    google_api_key=google_api_key,\n",
    "    working_dir=os.getcwd(),\n",
    "    documents=non_empty_documents\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
